# Copyright 2025 Cisco Systems, Inc. and its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# SPDX-License-Identifier: Apache-2.0

name: Cisco MCP Scanner - Comprehensive CI/CD

on:
  push:
    branches: [main, develop, 'release-*']
  pull_request:
    branches: [main, develop]
  workflow_dispatch:
    inputs:
      run_integration_tests:
        description: 'Run integration tests with live LLM'
        required: false
        default: false
        type: boolean
      run_e2e_tests:
        description: 'Run end-to-end tests'
        required: false
        default: false
        type: boolean

env:
  PYTHON_DEFAULT_VERSION: "3.13"

jobs:
  # ============================================================================
  # Stage 1: Unit Tests - Multi-Python Version Matrix
  # ============================================================================
  unit-tests:
    name: Unit Tests (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ["3.11", "3.12", "3.13"]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v6.7.0

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: uv sync --group dev

      - name: Run unit tests with coverage
        run: |
          uv run pytest tests/ \
            --ignore=tests/test_bedrock_integration.py \
            --ignore=tests/test_openai_llm_integration.py \
            -v \
            --cov=mcpscanner \
            --cov-report=xml \
            --cov-report=html \
            --cov-report=term-missing \
            --junitxml=test-results-${{ matrix.python-version }}.xml

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report-${{ matrix.python-version }}
          path: |
            coverage.xml
            htmlcov/
          retention-days: 7

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}
          path: test-results-${{ matrix.python-version }}.xml
          retention-days: 7

  # ============================================================================
  # Stage 4: Analyzer-Specific Tests
  # ============================================================================
  analyzer-tests:
    name: Analyzer Tests
    runs-on: ubuntu-latest
    needs: [unit-tests]
    strategy:
      fail-fast: false
      matrix:
        analyzer:
          - name: "YARA Analyzer"
            test_file: "tests/test_yara_analyzer.py"
          - name: "API Analyzer"
            test_file: "tests/test_api_analyzer.py"
          - name: "LLM Analyzer"
            test_file: "tests/test_llm_analyzer.py"
          - name: "Behavioral - Code Analyzer"
            test_file: "tests/behavioral/test_code_analyzer.py"
          - name: "Behavioral - Alignment Orchestrator"
            test_file: "tests/behavioral/test_alignment_orchestrator.py"
          - name: "Behavioral - Threat Mapper"
            test_file: "tests/behavioral/test_threat_mapper.py"
          - name: "Behavioral - Prompt Builder"
            test_file: "tests/behavioral/test_prompt_builder.py"
          - name: "Behavioral - Integration"
            test_file: "tests/behavioral/test_integration.py"
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v6.7.0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

      - name: Install dependencies
        run: uv sync --group dev

      - name: Run ${{ matrix.analyzer.name }} tests
        run: |
          uv run pytest ${{ matrix.analyzer.test_file }} -v --tb=short

  # ============================================================================
  # Stage 5: Static Analysis Tests
  # ============================================================================
  static-analysis-tests:
    name: Static Analysis Tests
    runs-on: ubuntu-latest
    needs: [unit-tests]
    strategy:
      fail-fast: false
      matrix:
        test:
          - name: "Parser"
            test_file: "tests/static_analysis/test_parser.py"
          - name: "Context Extractor"
            test_file: "tests/static_analysis/test_context_extractor.py"
          - name: "Dataflow"
            test_file: "tests/static_analysis/test_dataflow.py"
          - name: "Cross-File Dataflow"
            test_file: "tests/static_analysis/test_cross_file_dataflow.py"
          - name: "CFG"
            test_file: "tests/static_analysis/test_cfg.py"
          - name: "Taint Analysis"
            test_file: "tests/static_analysis/test_taint.py"
          - name: "Interprocedural"
            test_file: "tests/static_analysis/test_interprocedural.py"
          - name: "Semantic"
            test_file: "tests/static_analysis/test_semantic.py"
          - name: "Types"
            test_file: "tests/static_analysis/test_types.py"
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v6.7.0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

      - name: Install dependencies
        run: uv sync --group dev

      - name: Run ${{ matrix.test.name }} tests
        run: |
          uv run pytest ${{ matrix.test.test_file }} -v --tb=short

  # ============================================================================
  # Stage 6: Core Component Tests
  # ============================================================================
  core-tests:
    name: Core Component Tests
    runs-on: ubuntu-latest
    needs: [unit-tests]
    strategy:
      fail-fast: false
      matrix:
        component:
          - name: "Scanner"
            test_file: "tests/test_scanner.py"
          - name: "CLI"
            test_file: "tests/test_cli.py"
          - name: "Config"
            test_file: "tests/test_config.py"
          - name: "Config Parser"
            test_file: "tests/test_config_parser.py"
          - name: "Models"
            test_file: "tests/test_models.py"
          - name: "Base Analyzer"
            test_file: "tests/test_base.py"
          - name: "Authentication"
            test_file: "tests/test_auth.py"
          - name: "Report Generator"
            test_file: "tests/test_report_generator.py"
          - name: "Server API"
            test_file: "tests/test_server.py"
          - name: "Instructions Scanning"
            test_file: "tests/test_instructions_scanning.py"
          - name: "Custom Variable MCP"
            test_file: "tests/test_custom_variable_mcp.py"
          - name: "Expand Vars"
            test_file: "tests/test_expand_vars_default.py"
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v6.7.0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

      - name: Install dependencies
        run: uv sync --group dev

      - name: Run ${{ matrix.component.name }} tests
        run: |
          uv run pytest ${{ matrix.component.test_file }} -v --tb=short

  # ============================================================================
  # Stage 7: Integration Tests (Optional - requires secrets)
  # ============================================================================
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, analyzer-tests]
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.run_integration_tests == true
    env:
      MCP_SCANNER_LLM_API_KEY: ${{ secrets.MCP_SCANNER_LLM_API_KEY }}
      MCP_SCANNER_API_KEY: ${{ secrets.MCP_SCANNER_API_KEY }}
      MCP_SCANNER_ENDPOINT: ${{ secrets.MCP_SCANNER_ENDPOINT }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v6.7.0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

      - name: Install dependencies
        run: uv sync --group dev

      - name: Run OpenAI LLM Integration Tests
        if: env.MCP_SCANNER_LLM_API_KEY != ''
        env:
          MCP_SCANNER_LLM_MODEL: "gpt-4o"
        run: |
          uv run pytest tests/test_openai_llm_integration.py -v -m integration

      - name: Run Cisco AI Defense API Integration Tests
        if: env.MCP_SCANNER_API_KEY != ''
        env:
          MCP_SCANNER_ENDPOINT: ${{ env.MCP_SCANNER_ENDPOINT || 'https://us.api.inspect.aidefense.security.cisco.com/api/v1' }}
        run: |
          uv run pytest tests/test_api_analyzer.py -v -m integration

  # ============================================================================
  # Stage 8: MCP Scanner Live Tests - Run Scanner Against Example Servers
  # ============================================================================
  scanner-live-tests:
    name: MCP Scanner Live Tests
    runs-on: ubuntu-latest
    needs: [unit-tests]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v6.7.0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

      - name: Install dependencies
        run: uv sync --group dev

      - name: Install package
        run: uv pip install -e .

      # ----------------------------------------------------------------------
      # Test 1: Scan Malicious Streamable HTTP Server with YARA
      # ----------------------------------------------------------------------
      - name: Start Malicious MCP Server (Streamable HTTP)
        run: |
          uv run python examples/example-malicious-servers/malicious_mcp_streamable_server.py &
          sleep 5
          echo "Malicious server started on port 8000"

      - name: Wait for malicious server
        run: |
          for i in {1..30}; do
            if curl -s http://127.0.0.1:8000/mcp > /dev/null 2>&1; then
              echo "‚úÖ Malicious MCP server is ready"
              break
            fi
            echo "Waiting for server... ($i/30)"
            sleep 1
          done

      - name: "üîç YARA Scan - Malicious Server (expect threats detected)"
        run: |
          echo "================================================================================"
          echo "üîç SCANNING MALICIOUS SERVER WITH YARA ANALYZER"
          echo "================================================================================"
          
          # Run scanner and capture output
          OUTPUT=$(uv run mcp-scanner --analyzers yara --format detailed remote --server-url http://127.0.0.1:8000/mcp 2>&1) || true
          echo "$OUTPUT"
          
          # Verify threats were detected (malicious server should NOT be safe)
          if echo "$OUTPUT" | grep -qi "safe: no\|unsafe\|HIGH\|threat"; then
            echo ""
            echo "‚úÖ SUCCESS: YARA analyzer correctly detected threats in malicious server"
          else
            echo ""
            echo "‚ö†Ô∏è WARNING: Expected threats to be detected in malicious server"
          fi

      - name: "üîç YARA Scan - All Output Formats"
        run: |
          echo "--- Summary Format ---"
          uv run mcp-scanner --analyzers yara --format summary remote --server-url http://127.0.0.1:8000/mcp || true
          
          echo ""
          echo "--- Table Format ---"
          uv run mcp-scanner --analyzers yara --format table remote --server-url http://127.0.0.1:8000/mcp || true
          
          echo ""
          echo "--- Raw JSON Format ---"
          uv run mcp-scanner --analyzers yara --raw remote --server-url http://127.0.0.1:8000/mcp || true

      - name: Stop malicious server
        run: pkill -f "malicious_mcp_streamable_server.py" || true

      # ----------------------------------------------------------------------
      # Test 2: Scan Complete Test Server (mix of safe and malicious tools)
      # ----------------------------------------------------------------------
      - name: Start Complete Test Server
        run: |
          uv run python examples/mcp_complete_server.py &
          sleep 5
          echo "Complete test server started"

      - name: Wait for complete test server
        run: |
          for i in {1..30}; do
            if curl -s http://127.0.0.1:8000/mcp > /dev/null 2>&1; then
              echo "‚úÖ Complete test server is ready"
              break
            fi
            echo "Waiting for server... ($i/30)"
            sleep 1
          done

      - name: "üîç YARA Scan - Complete Server (Tools)"
        run: |
          echo "================================================================================"
          echo "üîç SCANNING COMPLETE SERVER - TOOLS"
          echo "================================================================================"
          uv run mcp-scanner --analyzers yara --format detailed remote --server-url http://127.0.0.1:8000/mcp || true

      - name: "üîç YARA Scan - Complete Server (Prompts)"
        run: |
          echo "================================================================================"
          echo "üîç SCANNING COMPLETE SERVER - PROMPTS"
          echo "================================================================================"
          uv run mcp-scanner --analyzers yara --format detailed prompts --server-url http://127.0.0.1:8000/mcp || true

      - name: "üîç YARA Scan - Complete Server (Resources)"
        run: |
          echo "================================================================================"
          echo "üîç SCANNING COMPLETE SERVER - RESOURCES"
          echo "================================================================================"
          uv run mcp-scanner --analyzers yara --format detailed resources --server-url http://127.0.0.1:8000/mcp || true

      - name: "üîç YARA Scan - Complete Server (Instructions)"
        run: |
          echo "================================================================================"
          echo "üîç SCANNING COMPLETE SERVER - INSTRUCTIONS"
          echo "================================================================================"
          uv run mcp-scanner --analyzers yara instructions --server-url http://127.0.0.1:8000/mcp || true

      - name: Stop complete test server
        run: pkill -f "mcp_complete_server.py" || true

      # ----------------------------------------------------------------------
      # Test 3: Behavioral Analyzer - Scan Malicious Source Code
      # ----------------------------------------------------------------------
      - name: "üîç Behavioral Scan - Malicious Server Source Code"
        run: |
          echo "================================================================================"
          echo "üîç BEHAVIORAL ANALYSIS - MALICIOUS SERVER SOURCE CODE"
          echo "================================================================================"
          
          # Scan the malicious behavioral server source code
          uv run mcp-scanner behavioral examples/example-malicious-servers/example_behavioural_malicious_server.py --format detailed || true
          
          echo ""
          echo "--- Scanning all malicious server examples ---"
          uv run mcp-scanner behavioral examples/example-malicious-servers/ --format summary || true

      # ----------------------------------------------------------------------
      # Test 4: API Server Tests
      # ----------------------------------------------------------------------
      - name: Start MCP Scanner API Server
        run: |
          uv run mcp-scanner-api --port 8001 &
          sleep 3

      - name: Start test MCP server for API tests
        run: |
          uv run python examples/example-malicious-servers/malicious_mcp_streamable_server.py &
          sleep 5

      - name: Wait for both servers
        run: |
          for i in {1..30}; do
            if curl -s http://127.0.0.1:8001/health > /dev/null 2>&1; then
              echo "‚úÖ API server is ready"
              break
            fi
            sleep 1
          done
          
          for i in {1..30}; do
            if curl -s http://127.0.0.1:8000/mcp > /dev/null 2>&1; then
              echo "‚úÖ MCP server is ready"
              break
            fi
            sleep 1
          done

      - name: "üîç API Test - Health Check"
        run: |
          echo "================================================================================"
          echo "üîç API SERVER - HEALTH CHECK"
          echo "================================================================================"
          curl -s http://127.0.0.1:8001/health | python -m json.tool

      - name: "üîç API Test - Scan All Tools"
        run: |
          echo "================================================================================"
          echo "üîç API SERVER - SCAN ALL TOOLS"
          echo "================================================================================"
          curl -s -X POST http://127.0.0.1:8001/scan-all-tools \
            -H "Content-Type: application/json" \
            -d '{"server_url": "http://127.0.0.1:8000/mcp", "analyzers": ["yara"]}' | python -m json.tool || true

      - name: "üîç API Test - Scan Specific Tool"
        run: |
          echo "================================================================================"
          echo "üîç API SERVER - SCAN SPECIFIC TOOL (execute_system_command)"
          echo "================================================================================"
          curl -s -X POST http://127.0.0.1:8001/scan-tool \
            -H "Content-Type: application/json" \
            -d '{"server_url": "http://127.0.0.1:8000/mcp", "tool_name": "execute_system_command", "analyzers": ["yara"]}' | python -m json.tool || true

      - name: "üîç API Test - Scan All Prompts"
        run: |
          echo "================================================================================"
          echo "üîç API SERVER - SCAN ALL PROMPTS"
          echo "================================================================================"
          curl -s -X POST http://127.0.0.1:8001/scan-all-prompts \
            -H "Content-Type: application/json" \
            -d '{"server_url": "http://127.0.0.1:8000/mcp", "analyzers": ["yara"]}' | python -m json.tool || true

      - name: "üîç API Test - Scan Instructions"
        run: |
          echo "================================================================================"
          echo "üîç API SERVER - SCAN INSTRUCTIONS"
          echo "================================================================================"
          curl -s -X POST http://127.0.0.1:8001/scan-instructions \
            -H "Content-Type: application/json" \
            -d '{"server_url": "http://127.0.0.1:8000/mcp", "analyzers": ["yara"]}' | python -m json.tool || true

      - name: Cleanup servers
        if: always()
        run: |
          pkill -f "mcp-scanner-api" || true
          pkill -f "malicious_mcp_streamable_server.py" || true
          pkill -f "mcp_complete_server.py" || true

  # ============================================================================
  # Stage 9: End-to-End Tests with LLM (Optional - requires secrets)
  # ============================================================================
  e2e-tests-llm:
    name: E2E Tests with LLM Analyzer
    runs-on: ubuntu-latest
    needs: [scanner-live-tests]
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.run_e2e_tests == true
    env:
      MCP_SCANNER_LLM_API_KEY: ${{ secrets.MCP_SCANNER_LLM_API_KEY }}
      MCP_SCANNER_LLM_MODEL: ${{ secrets.MCP_SCANNER_LLM_MODEL }}
      MCP_SCANNER_API_KEY: ${{ secrets.MCP_SCANNER_API_KEY }}
      MCP_SCANNER_ENDPOINT: ${{ secrets.MCP_SCANNER_ENDPOINT }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v6.7.0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

      - name: Install dependencies
        run: uv sync --group dev

      - name: Install package
        run: uv pip install -e .

      - name: Start Malicious MCP Server
        run: |
          uv run python examples/example-malicious-servers/malicious_mcp_streamable_server.py &
          sleep 5

      - name: Wait for server
        run: |
          for i in {1..30}; do
            if curl -s http://127.0.0.1:8000/mcp > /dev/null 2>&1; then
              echo "‚úÖ Server is ready"
              break
            fi
            sleep 1
          done

      - name: "üîç LLM Scan - Malicious Server"
        if: env.MCP_SCANNER_LLM_API_KEY != ''
        env:
          MCP_SCANNER_LLM_MODEL: ${{ env.MCP_SCANNER_LLM_MODEL || 'gpt-4o' }}
        run: |
          echo "================================================================================"
          echo "üîç LLM ANALYZER - SCANNING MALICIOUS SERVER"
          echo "================================================================================"
          uv run mcp-scanner --analyzers llm --format detailed remote --server-url http://127.0.0.1:8000/mcp || true

      - name: "üîç All Analyzers Scan - Malicious Server"
        if: env.MCP_SCANNER_LLM_API_KEY != '' && env.MCP_SCANNER_API_KEY != ''
        env:
          MCP_SCANNER_ENDPOINT: ${{ env.MCP_SCANNER_ENDPOINT || 'https://us.api.inspect.aidefense.security.cisco.com/api/v1' }}
          MCP_SCANNER_LLM_MODEL: ${{ env.MCP_SCANNER_LLM_MODEL || 'gpt-4o' }}
        run: |
          echo "================================================================================"
          echo "üîç ALL ANALYZERS (API + YARA + LLM) - SCANNING MALICIOUS SERVER"
          echo "================================================================================"
          uv run mcp-scanner --analyzers api yara llm --format detailed remote --server-url http://127.0.0.1:8000/mcp || true

      - name: Cleanup
        if: always()
        run: pkill -f "malicious_mcp_streamable_server.py" || true

  # ============================================================================
  # Stage 9: Documentation Verification
  # ============================================================================
  docs:
    name: Documentation Check
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check documentation files exist
        run: |
          # Verify essential documentation exists
          DOCS=(
            "README.md"
            "docs/README.md"
            "docs/architecture.md"
            "docs/llm-providers.md"
            "docs/mcp-threats-taxonomy.md"
            "docs/authentication.md"
            "docs/api-reference.md"
            "docs/output-formats.md"
            "docs/programmatic-usage.md"
          )
          
          MISSING=0
          for doc in "${DOCS[@]}"; do
            if [ -f "$doc" ]; then
              echo "‚úÖ $doc exists"
            else
              echo "‚ùå $doc is missing"
              MISSING=$((MISSING + 1))
            fi
          done
          
          if [ $MISSING -gt 0 ]; then
            echo "‚ö†Ô∏è $MISSING documentation file(s) missing"
            exit 1
          fi
          echo "‚úÖ All documentation files present"

      - name: Check for broken internal links
        run: |
          # Simple check for broken markdown links to local files
          find docs -name "*.md" -exec grep -l '\[.*\](.*\.md)' {} \; | while read file; do
            echo "Checking links in $file"
          done

  # ============================================================================
  # Stage 11: YARA Rules Validation
  # ============================================================================
  yara-validation:
    name: YARA Rules Validation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v6.7.0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_DEFAULT_VERSION }}

      - name: Install dependencies
        run: uv sync --group dev

      - name: Validate YARA rules syntax
        run: |
          uv run python -c "
          import yara
          import os
          from pathlib import Path
          
          rules_dir = Path('mcpscanner/data/yara_rules')
          errors = []
          
          for rule_file in rules_dir.glob('*.yar*'):
              try:
                  yara.compile(filepath=str(rule_file))
                  print(f'‚úÖ {rule_file.name}: Valid')
              except yara.SyntaxError as e:
                  errors.append(f'{rule_file.name}: {e}')
                  print(f'‚ùå {rule_file.name}: {e}')
          
          if errors:
              print(f'\n{len(errors)} YARA rule(s) have syntax errors')
              exit(1)
          else:
              print(f'\nAll YARA rules are valid')
          "

  # ============================================================================
  # Stage 12: Prompt Templates Validation
  # ============================================================================
  prompt-validation:
    name: Prompt Templates Validation
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Validate prompt templates exist and are non-empty
        run: |
          for file in mcpscanner/data/prompts/*.md; do
            if [ -f "$file" ]; then
              if [ -s "$file" ]; then
                echo "‚úÖ $file: Valid ($(wc -l < "$file") lines)"
              else
                echo "‚ùå $file: Empty file"
                exit 1
              fi
            fi
          done

  # ============================================================================
  # Stage 14: Final Summary
  # ============================================================================
  ci-summary:
    name: CI Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, analyzer-tests, static-analysis-tests, core-tests, scanner-live-tests, docs, yara-validation, prompt-validation]
    if: always()
    steps:
      - name: Check job statuses
        run: |
          echo "================================================================================"
          echo "üìä CISCO MCP SCANNER - CI/CD PIPELINE SUMMARY"
          echo "================================================================================"
          echo ""
          echo "Job Results:"
          echo "  - Unit Tests: ${{ needs.unit-tests.result }}"
          echo "  - Analyzer Tests: ${{ needs.analyzer-tests.result }}"
          echo "  - Static Analysis Tests: ${{ needs.static-analysis-tests.result }}"
          echo "  - Core Tests: ${{ needs.core-tests.result }}"
          echo "  - Scanner Live Tests: ${{ needs.scanner-live-tests.result }}"
          echo "  - Docs: ${{ needs.docs.result }}"
          echo "  - YARA Validation: ${{ needs.yara-validation.result }}"
          echo "  - Prompt Validation: ${{ needs.prompt-validation.result }}"
          echo ""
          
          # Determine overall status
          if [[ "${{ needs.unit-tests.result }}" == "failure" ]] || \
             [[ "${{ needs.analyzer-tests.result }}" == "failure" ]] || \
             [[ "${{ needs.static-analysis-tests.result }}" == "failure" ]] || \
             [[ "${{ needs.core-tests.result }}" == "failure" ]] || \
             [[ "${{ needs.scanner-live-tests.result }}" == "failure" ]]; then
            echo "‚ùå Pipeline FAILED - Critical jobs failed"
            exit 1
          else
            echo "‚úÖ Pipeline PASSED - All critical jobs succeeded"
          fi
