# MCP Scanner Evaluation Suite

This directory contains evaluation datasets and scripts for testing the MCP Scanner's detection capabilities.

## Structure

```
evals/
â””â”€â”€ behavioral-analysis/
    â”œâ”€â”€ data/           # Malicious MCP server test cases (141 files)
    â”‚   â”œâ”€â”€ arbitrary-resource-read-write/
    â”‚   â”œâ”€â”€ backdoor/
    â”‚   â”œâ”€â”€ data-exfiltration/
    â”‚   â”œâ”€â”€ defense-evasion/
    â”‚   â”œâ”€â”€ general-description-code-mismatch/
    â”‚   â”œâ”€â”€ goal-manipulation/
    â”‚   â”œâ”€â”€ injection-attacks/
    â”‚   â”œâ”€â”€ prompt-injection/
    â”‚   â”œâ”€â”€ resource-exhaustion/
    â”‚   â”œâ”€â”€ template-injection/
    â”‚   â”œâ”€â”€ tool-poisoning/
    â”‚   â”œâ”€â”€ unauthorized-code-execution/
    â”‚   â”œâ”€â”€ unauthorized-network-access/
    â”‚   â””â”€â”€ unauthorized-system-access/
    â””â”€â”€ scripts/
        â”œâ”€â”€ run_behavioral_scan.py
        â””â”€â”€ scan_results.json (generated)
```

## Behavioral Analysis Evaluation

The behavioral analysis evaluation tests the scanner's ability to detect malicious patterns in MCP server source code through static analysis and LLM-powered alignment checking.

### Prerequisites

The behavioral analyzer requires LLM access. Set up your environment:

```bash
# For Azure OpenAI
export MCP_SCANNER_LLM_API_KEY="your_api_key"
export MCP_SCANNER_LLM_MODEL="azure/gpt-5.2"
export MCP_SCANNER_LLM_BASE_URL="https://your-endpoint.openai.azure.com/"
export MCP_SCANNER_LLM_API_VERSION="2024-02-15-preview"

# For OpenAI
export MCP_SCANNER_LLM_API_KEY="your_openai_key"
export MCP_SCANNER_LLM_MODEL="gpt-4o"

# For AWS Bedrock
export AWS_PROFILE="your-profile"
export AWS_REGION="us-east-1"
export MCP_SCANNER_LLM_MODEL="bedrock/anthropic.claude-sonnet-4-5-20250929-v2:0"
```

### Running the Evaluation

Navigate to the scripts directory and run:

```bash
cd evals/behavioral-analysis/scripts
uv run python run_behavioral_scan.py
```

### Example Output

```
================================================================================
Behavioral Analysis Evaluation Scanner
================================================================================

ðŸ“‚ Data directory: /path/to/evals/behavioral-analysis/data
ðŸ¤– LLM Model: azure/gpt-4.1
ðŸ“Š Found 14 threat categories

ðŸ“ Scanning arbitrary-resource-read-write: 10 files
  ðŸ” arbitrary_file_copy_sensitive_data.py... âœ… DETECTED (3 findings)
  ðŸ” arbitrary_file_deletion_recursive.py... âœ… DETECTED (2 findings)
  ðŸ” path_traversal_directory_enumeration.py... âœ… DETECTED (4 findings)
  ...

ðŸ“ Scanning backdoor: 10 files
  ðŸ” dns_tunneling_c2_communication.py... âœ… DETECTED (5 findings)
  ðŸ” multi_layer_obfuscated_backdoor.py... âœ… DETECTED (3 findings)
  ...

================================================================================
SUMMARY
================================================================================
Total files scanned: 141
âœ… Detected (with findings): 135
âš ï¸  Missed (no findings): 5
âŒ Errors: 1

ðŸŽ¯ Detection Rate: 95.7%

ðŸ’¾ Detailed results saved to: scan_results.json
================================================================================
```

### Understanding Results

**Status Indicators:**
- âœ… **DETECTED** - Behavioral analyzer found security findings
- âš ï¸ **MISSED** - No findings detected (potential false negative)
- âŒ **ERROR** - Analysis failed (check error details in JSON)

**Output Files:**
- `scan_results.json` - Detailed results with all findings per file

### Results Format

The `scan_results.json` file contains:

```json
{
  "summary": {
    "total_files": 141,
    "detected": 135,
    "missed": 5,
    "errors": 1,
    "detection_rate": "95.7%"
  },
  "results_by_category": {
    "arbitrary-resource-read-write": [
      {
        "file": "data/arbitrary-resource-read-write/arbitrary_file_copy_sensitive_data.py",
        "status": "completed",
        "is_safe": false,
        "findings_count": 3,
        "findings": [
          {
            "severity": "high",
            "summary": "Arbitrary file read/write without validation",
            "threat_category": "arbitrary-resource-read-write"
          }
        ]
      }
    ]
  }
}
```

## Threat Categories

### Data Directory

The `data/` directory contains 141 malicious MCP server implementations organized by threat type:

| Category | Description | Files |
|----------|-------------|-------|
| **arbitrary-resource-read-write** | File system manipulation attacks | 10 |
| **backdoor** | Backdoor and persistence mechanisms | 10 |
| **data-exfiltration** | Data theft and credential harvesting | 11 |
| **defense-evasion** | Anti-analysis and evasion techniques | 10 |
| **general-description-code-mismatch** | Misleading documentation | 10 |
| **goal-manipulation** | Goal hijacking and manipulation | 10 |
| **injection-attacks** | Various injection vulnerabilities | 10 |
| **prompt-injection** | Prompt injection patterns | 10 |
| **resource-exhaustion** | DoS and resource exhaustion | 10 |
| **template-injection** | Template injection vulnerabilities | 10 |
| **tool-poisoning** | Tool manipulation and poisoning | 10 |
| **unauthorized-code-execution** | Arbitrary code execution | 10 |
| **unauthorized-network-access** | SSRF and network attacks | 10 |
| **unauthorized-system-access** | System information disclosure | 10 |

## Adding New Test Cases

To add new malicious server implementations:

1. Choose the appropriate threat category in `data/`
2. Create a new Python file with a descriptive name
3. Implement the malicious MCP server pattern
4. Add Apache 2.0 license header
5. Run the evaluation script to verify detection

Example:

```bash
# Create new test case
cat > evals/behavioral-analysis/data/backdoor/my_new_backdoor.py << 'EOF'
# Copyright 2025 Cisco Systems, Inc. and its affiliates
# SPDX-License-Identifier: Apache-2.0

"""MCP server with hidden backdoor functionality."""

from mcp.server import Server

# Your malicious implementation here
EOF

# Run evaluation
cd evals/behavioral-analysis/scripts
uv run python run_behavioral_scan.py
```

## Troubleshooting

### LLM Configuration Errors

If you see:
```
âŒ Error: LLM configuration required for behavioral analysis
```

Ensure you've set the required environment variables (see Prerequisites above).

### Analysis Timeouts

For large files or complex analysis, you may need to increase the timeout:

```bash
export MCP_SCANNER_LLM_TIMEOUT=300  # 5 minutes
```

### Rate Limiting

If you encounter rate limits, consider:
- Using a higher-tier API plan
- Adding delays between requests
- Running on a subset of files first

## Performance Notes

- **Average time per file**: 10-30 seconds (depends on LLM provider)
- **Total runtime for 141 files**: ~30-60 minutes
- **Recommended**: Run during off-peak hours or use batch processing

## Contributing

When adding new evaluation test cases:

1. Ensure the malicious pattern is realistic and represents real threats
2. Add clear comments explaining the malicious behavior
3. Include proper license headers
4. Test that the behavioral analyzer detects the pattern
5. Document any special setup or dependencies

## License

All evaluation test cases are licensed under Apache 2.0.
